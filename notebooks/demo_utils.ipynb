{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian Calculation thorugh hessian-vector Product Example\n",
    "Purpose: \n",
    "- To demonstrate the usage of `hessian_fun` and `gradient_fun` from `pytorchpruner.utils`.\n",
    "- These functions are also used as tests/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "import sys\n",
    "sys.path.insert(0,\"../\")\n",
    "from pytorchpruner.utils import hessian_fun,gradient_fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first compare the gradient, as you can see the autograd corrrectly calculates the gradient **G(w)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      " 0.6152\n",
      " 0.2209\n",
      " 0.9314\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "The difference is:  0.0\n"
     ]
    }
   ],
   "source": [
    "# We have 3 random parameters\n",
    "w = Parameter(torch.rand(3))\n",
    "print(w)\n",
    "\n",
    "def L(w):\n",
    "    #A custom loss function\n",
    "    return (w[0]**2)*w[1]+4*(w[2]**3)*w[0]\n",
    "\n",
    "def G(w):\n",
    "    #Jacobian of the L(w)\n",
    "    wd = w.data\n",
    "    return torch.Tensor([2*wd[0]*wd[1]+4*(wd[2]**3),\n",
    "                         wd[0]**2,\n",
    "                        12*(wd[2]**2)*wd[0]])\n",
    "\n",
    "#torch.autograd\n",
    "loss_val = L(w)\n",
    "autograd_grad = gradient_fun(loss_val, w).data\n",
    "#emprical gradient\n",
    "correct_grad = G(w)\n",
    "print('The difference is: ',(autograd_grad-correct_grad).abs().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      " 0.9894  0.6487\n",
      " 0.8904  0.2573\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "\n",
      " 10.4586   0.0000\n",
      "  4.3320   0.7928\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "\n",
      " 10.4586   0.0000\n",
      "  4.3320   0.7928\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "The difference is:  0.0\n"
     ]
    }
   ],
   "source": [
    "# We have 4 random parameters\n",
    "w = Parameter(torch.rand(2,2))\n",
    "print(w)\n",
    "\n",
    "def L2d(w):\n",
    "    '''\n",
    "    A custom loss function\n",
    "        w00 w01\n",
    "        w10 w11\n",
    "    '''    \n",
    "    return (w[1,0]**2)*w[1,1]+4*(w[0,0]**3)*w[1,0]\n",
    "\n",
    "def G2d(w):\n",
    "    #Jacobian of the L(w)\n",
    "    wd = w.data\n",
    "    return torch.Tensor([\n",
    "                         [12*(wd[0,0]**2)*wd[1,0],\n",
    "                          0],\n",
    "                         [2*wd[1,0]*wd[1,1]+4*(wd[0,0]**3),\n",
    "                          wd[1,0]**2]\n",
    "                        ])\n",
    "\n",
    "#torch.autograd\n",
    "loss_val = L2d(w)\n",
    "autograd_grad = gradient_fun(loss_val, w).data\n",
    "print(autograd_grad)\n",
    "#emprical gradient\n",
    "correct_grad = G2d(w)\n",
    "print(correct_grad)\n",
    "print('The difference is: ',(autograd_grad-correct_grad).abs().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now lets calculates hessian through hessian vector product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1.1133  1.8402  0.3115\n",
      " 1.8402  0.0000  0.0000\n",
      " 0.3115  0.0000  3.5577\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "w = Parameter(torch.rand(3))\n",
    "\n",
    "def H(w):\n",
    "    #Hessian of the L(w)\n",
    "    wd=w.data\n",
    "    gw12 = 2*wd[0]\n",
    "    gw13 = 12*(wd[2]**2)\n",
    "    gw23 = 0\n",
    "    gw11 = 2*wd[1]\n",
    "    gw22 = 0\n",
    "    gw33 = 24*wd[2]*wd[0]  \n",
    "    return torch.Tensor([[gw11,gw12,gw13],\n",
    "                         [gw12,gw22,gw23],\n",
    "                         [gw13,gw23,gw33]])\n",
    "    \n",
    "a = L(w)\n",
    "hessian = hessian_fun(a,w)\n",
    "    \n",
    "print(hessian)\n",
    "print(torch.sum(torch.abs(hessian-H(w))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,0 ,.,.) = \n",
      "  0.4570  0.0000\n",
      "  0.0136  0.0000\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      "  0.0000  0.0000\n",
      "  0.0000  0.0000\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "  0.0136  0.0000\n",
      "  1.4941  1.1315\n",
      "\n",
      "(1 ,1 ,.,.) = \n",
      "  0.0000  0.0000\n",
      "  1.1315  0.0000\n",
      "[torch.FloatTensor of size 2x2x2x2]\n",
      "\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "w = Parameter(torch.rand(2,2))\n",
    "    \n",
    "def H2d(w):\n",
    "    #Hessian of the L(w)\n",
    "    wd=w.data\n",
    "    gw12 = 2*wd[1,0]\n",
    "    gw13 = 12*(wd[0,0]**2)\n",
    "    gw23 = 0\n",
    "    gw11 = 2*wd[1,1]\n",
    "    gw22 = 0\n",
    "    gw33 = 24*wd[0,0]*wd[1,0] \n",
    "    ## x3 0\n",
    "    ## x1 x2\n",
    "    return torch.Tensor([[[[gw33,0], #0,0,0,:\n",
    "                           [gw13,gw23]], #0,0,1,: #x3 with others\n",
    "                          [[0,0], #0,1,0,:\n",
    "                           [0,0]]], #0,1,1,: \n",
    "                         [[[gw13,0], #1,0,0,:\n",
    "                           [gw11,gw12]], #1,0,1,: #x1 with others\n",
    "                          [[gw23,0], #1,1,0,:\n",
    "                          [gw12,gw22]]]]) #1,1,1,: #x2 with others\n",
    "\n",
    "a = L2d(w)\n",
    "hessian = hessian_fun(a,w)\n",
    "\n",
    "print(torch.stack(hessian))\n",
    "print(torch.sum(torch.abs(hessian-H2d(w))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try selecting some parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.00000e-02 *\n",
      "  3.2827\n",
      "  6.8760\n",
      "  1.9848\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "The difference is:  5.587935447692871e-09\n"
     ]
    }
   ],
   "source": [
    "# We have 3 random parameters\n",
    "genesis = torch.rand(3)\n",
    "w = Parameter(genesis)\n",
    "w2 = Parameter(genesis)\n",
    "\n",
    "def L2(w,w2):\n",
    "    #A custom loss function\n",
    "    return (w[0]**2)*w[1]+4*(w[2]**3)*w[0]+(w2[0]**2)*w2[1]+4*(w2[2]**3)*w2[0]\n",
    "\n",
    "def G2(w,w2):\n",
    "    #Jacobian of the L(w)\n",
    "    wd = w.data\n",
    "    wd2 = w2.data\n",
    "    return (torch.Tensor([2*wd[0]*wd[1]+4*(wd[2]**3),\n",
    "                         wd[0]**2,\n",
    "                        12*(wd[2]**2)*wd[0]]),\n",
    "            torch.Tensor([2*wd2[0]*wd2[1]+4*(wd2[2]**3),\n",
    "                         wd2[0]**2,\n",
    "                        12*(wd2[2]**2)*wd2[0]])\n",
    "           )\n",
    "\n",
    "#torch.autograd\n",
    "loss_val = L2(w,w2)\n",
    "autograd_grad = gradient_fun(loss_val, w2).data\n",
    "#emprical gradient\n",
    "correct_grad = G2(w,w2)[1]\n",
    "print(autograd_grad)\n",
    "print('The difference is: ',(autograd_grad-correct_grad).abs().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,0 ,.,.) = \n",
      "  0.4570  0.0000\n",
      "  0.0136  0.0000\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      "  0.0000  0.0000\n",
      "  0.0000  0.0000\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "  0.0136  0.0000\n",
      "  1.4941  1.1315\n",
      "\n",
      "(1 ,1 ,.,.) = \n",
      "  0.0000  0.0000\n",
      "  1.1315  0.0000\n",
      "[torch.FloatTensor of size 2x2x2x2]\n",
      "\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# We have 3 random parameters\n",
    "genesis = torch.rand(3)\n",
    "w = Parameter(genesis)\n",
    "w2 = Parameter(genesis)\n",
    "\n",
    "def L2(w,w2):\n",
    "    #A custom loss function\n",
    "    return (w[0]**2)*w[1]+4*(w[2]**3)*w[0]+(w2[0]**2)*w2[1]+4*(w2[2]**3)*w2[0]\n",
    "\n",
    "def H2(w,w2):\n",
    "    #Hessian of the L(w)\n",
    "    wd=w.data\n",
    "    wd2 = w2.data\n",
    "    \n",
    "    gw12 = 2*wd[0]\n",
    "    gw13 = 12*(wd[2]**2)\n",
    "    gw23 = 0\n",
    "    gw11 = 2*wd[1]\n",
    "    gw22 = 0\n",
    "    gw33 = 24*wd[2]*wd[0]  \n",
    "    return (torch.Tensor([[gw11,gw12,gw13],\n",
    "                         [gw12,gw22,gw23],\n",
    "                         [gw13,gw23,gw33]]),\n",
    "            torch.Tensor([[gw11,gw12,gw13],\n",
    "                         [gw12,gw22,gw23],\n",
    "                         [gw13,gw23,gw33]])\n",
    "           )\n",
    "    \n",
    "\n",
    "losses = L2(w,w2)\n",
    "hessian2 = hessian_fun(losses,w2)\n",
    "    \n",
    "print(hessian)\n",
    "print(torch.sum(torch.abs(hessian2-H2(w,w2)[1])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
